{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNB3j0pPk3F94jD5c65o/Fo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarelHeby/Deep-Learning---Chest-X-Ray/blob/main/Deep_Learning_Chest_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**FIRST SETUP**"
      ],
      "metadata": {
        "id": "G3B7bvb-5iWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQs7UYex44Fs"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# Create a Kaggle API token and save it to a file.\n",
        "!echo '{\"username\":\"barelheby\",\"key\":\"978bdd6cd1cc991c69bbe920fe75a9cc\"}' > kaggle.json\n",
        "\n",
        "# Copy the Kaggle API token file to the correct location.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "# Change the permissions of the Kaggle API token file.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset from Kaggle.\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "# Unzip the downloaded dataset.\n",
        "!unzip chest-xray-pneumonia.zip\n",
        "\n",
        "# Print the contents of the current working directory.\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**IMPORTS**"
      ],
      "metadata": {
        "id": "d7wq0ds96-sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "al1niDEz7ICS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        " **Configuration**\n"
      ],
      "metadata": {
        "id": "iEw76ZnW9wkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_BATCH_SIZE = 32\n",
        "TF_SEED = 123\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "DIR_DATASET = \"/content/chest_xray\""
      ],
      "metadata": {
        "id": "fU4yOUMT934-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Normalize Image Function**"
      ],
      "metadata": {
        "id": "_ktR0gZsHSrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Training Set Preprocess**"
      ],
      "metadata": {
        "id": "NQOLQigP7Reu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = join(DIR_DATASET,\"train\")\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    validation_split=0.2,\n",
        "    subset = \"training\",\n",
        "    seed = TF_SEED,\n",
        "    image_size = (IMG_HEIGHT,IMG_WIDTH),\n",
        "    batch_size = TF_BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MmsOH817dbC",
        "outputId": "4b26f659-2eeb-45fe-e067-38cb729b0b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Using 4173 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Validation Set Prep**"
      ],
      "metadata": {
        "id": "Ladr4s8i_PzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    validation_split=0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = TF_SEED,\n",
        "    image_size = (IMG_HEIGHT,IMG_WIDTH),\n",
        "    batch_size = TF_BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wachYFoC_aDu",
        "outputId": "66a05bc4-0cb4-4ce2-a779-67fae0dc5027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Using 1043 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8bcjU-_B8s9",
        "outputId": "5ec2860a-e7a4-4c0f-da06-0aed600f8fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NORMAL', 'PNEUMONIA']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded_class_names = [i for (i,label) in enumerate(train_dataset.class_names)]\n",
        "validation_encoded_class_names = [i for (i,label) in enumerate(validation_dataset.class_names)]\n"
      ],
      "metadata": {
        "id": "9KMHcbO_CO1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(lambda x,y:(x/255.0,y))\n",
        "validation_dataset = validation_dataset.map(lambda x,y:(x/255.0,y))"
      ],
      "metadata": {
        "id": "meqKwjioFjO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1 Model**"
      ],
      "metadata": {
        "id": "dnBYEXPnJ40x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.layers import BatchNormalization,Dropout,Dense,Flatten\n",
        "from tensorflow.keras.optimizers import Adamax,Adam\n"
      ],
      "metadata": {
        "id": "pOEXaQfVKHM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_network = VGG16(weights=\"imagenet\",include_top=False,input_shape = (IMG_HEIGHT,IMG_WIDTH,3),pooling=\"avg\")\n",
        "base_network.trainable = False"
      ],
      "metadata": {
        "id": "4vAnDbYkK_uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = Sequential(name=\"Assigment_1\")\n",
        "model.add(base_network)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation=\"relu\"))\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(60,activation=\"relu\"))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "model.compile(optimizer=Adam(),loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SOivLluzMHJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(train_dataset,\n",
        "                        epochs= 10,\n",
        "                        validation_data = validation_dataset,\n",
        "                        callbacks = early_stopping)"
      ],
      "metadata": {
        "id": "QgUx0ox5Nxhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loss, validation_accuracy = model.evaluate(validation_dataset)\n",
        "print(\"Validation Loss:\", validation_loss)\n",
        "print(\"Validation Accuracy:\", validation_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDlDvvpoUXZ6",
        "outputId": "8eb85fd3-e4e0-46ef-cae4-60a6c465cc17"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 11s 278ms/step - loss: 0.0786 - accuracy: 0.9741\n",
            "Validation Loss: 0.07857345789670944\n",
            "Validation Accuracy: 0.9741131067276001\n"
          ]
        }
      ]
    }
  ]
}